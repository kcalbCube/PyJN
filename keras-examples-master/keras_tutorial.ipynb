{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. はじめに\n",
    "機械学習勉強会#2でkerasの紹介用に作成しました．\n",
    "\n",
    "例として以下のネットワークが登場します．\n",
    "\n",
    "- image\n",
    "  - MLP with MNIST\n",
    "  - CNN with MNIST\n",
    "  - CNN with cifiar10\n",
    "  - autoencoder with MNIST\n",
    "- text\n",
    "  - RNN with binary classification of movie review\n",
    "  - encoder-decoder (machine translation model)\n",
    "  - NNLM (language model)\n",
    "  - Continuous Bag-of-Words\n",
    "  - Skip Gram\n",
    "\n",
    "個別のコードについては [github](https://github.com/nzw0301/keras-examples) にあります．\n",
    "\n",
    "## 0.1 実行環境\n",
    "\n",
    "- python 3.5.1 (anaconda3-4.0.0)\n",
    "   - keras 1.0.2\n",
    "   - theano 0.8.2\n",
    "   - pydot-ng 1.0.0\n",
    "   - gensim 0.12.4\n",
    "   - numpy 1.11.0\n",
    "   - daft 0.0.4\n",
    "- OSX (yosemite)\n",
    "- graphviz\n",
    "\n",
    "もし [`pyenv`](https://github.com/yyuu/pyenv) が導入済みであれば，以下のコマンドを実行して下さい．\n",
    "\n",
    "`$ pyenv install anaconda3-4.0.0; pip install keras theano gensim pydot-ng git+https://github.com/dfm/daft.git`\n",
    "\n",
    "\n",
    "## 0.2 概要\n",
    "\n",
    "### kerasの特徴 (公式docsより)\n",
    "- Modularity\n",
    "- Minimalism\n",
    "- Easy extensibility\n",
    "- Work with Python\n",
    "\n",
    "### そのほかkerasの特徴など\n",
    "\n",
    "  - kerasは，tensorFlowとtheanoという既存のライブラリのラッパー\n",
    "  - TFとtheanoを，そのまま使ってNNを記述するのはつらい\n",
    "  - kerasでは，どちらかをbackendとして利用\n",
    "  - CNNを使う場合は[TensorFlowのほうが速い](https://github.com/fchollet/keras/wiki/Keras,-now-running-on-TensorFlow#performance)\n",
    "  - TFやtheanoやchainerは，変数を専用のインスタンスに変換する必要があるが，kerasではそれを行う必要がない\n",
    "  - コミュニティの規模も大きめ https://twitter.com/fchollet/status/720639335524007937"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7e3f22a0c66d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdaft\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2vec\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSVG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import rc\n",
    "%matplotlib inline\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "import daft\n",
    "from gensim.models.doc2vec import Word2Vec\n",
    "import json\n",
    "from IPython.display import SVG, display\n",
    "import numpy as np\n",
    "np.random.seed(13)\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model, Sequential, model_from_json\n",
    "from keras.datasets import mnist, cifar10, imdb\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Embedding, LSTM, GRU, Input, RepeatVector, TimeDistributed, Merge, Lambda, Reshape\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.visualize_util import model_to_dot, plot\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer, base_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "def draw_digit(data, row, col, n):\n",
    "    plt.subplot(row, col, n)    \n",
    "    plt.imshow(data)\n",
    "    plt.gray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. MLP with MNIST\n",
    "\n",
    "- MNIST（0~9の手書き文字）を使用\n",
    "- 隠れ層1層の多層パーセプトロン\n",
    "- 層の記述方法について言及"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgm = daft.PGM(shape=[5, 5])\n",
    "\n",
    "# Nodes\n",
    "for i in range(5):\n",
    "    pgm.add_node(daft.Node(\"x\"+str(i), \"\", 1, i+0.5))\n",
    "\n",
    "for h in range(4):\n",
    "    pgm.add_node(daft.Node(\"h\"+str(h), \"\", 2.5, h+0.85))\n",
    "\n",
    "for o in range(5):\n",
    "    pgm.add_node(daft.Node(\"o\"+str(o), \"\", 4.0, o+0.5))\n",
    "\n",
    "# Edges\n",
    "for i in range(5):\n",
    "    for h in range(4):\n",
    "        pgm.add_edge(\"x\"+str(i), \"h\"+str(h))\n",
    "\n",
    "for h in range(4):\n",
    "    for o in range(5):\n",
    "        pgm.add_edge(\"h\"+str(h), \"o\"+str(o))\n",
    "\n",
    "        \n",
    "pgm.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 データ\n",
    "\n",
    "kerasの関数でダウンロードしたデータセットは `~/.keras/datasets/` 直下に配置されます．  \n",
    "これによって繰り返しダウンロードする必要がなくなります．  \n",
    "また，`MNIST`や`CIFAR10`のようなデータセットについては，あらかじめ関数が用意されています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ読み込み\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "nb_classes = 10 # class size\n",
    "input_unit_size = 28*28 # input vector size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_size = 10\n",
    "total = 0\n",
    "plt.figure(figsize=(20,20))\n",
    "for i in range(show_size):\n",
    "    for j in range(show_size):        \n",
    "        draw_digit(X_train[total], show_size, show_size, total+1)\n",
    "        total+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理\n",
    "X_train = X_train.reshape(X_train.shape[0], input_unit_size)\n",
    "X_test  = X_test.reshape(X_test.shape[0], input_unit_size)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test  = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test  /= 255\n",
    "\n",
    "# one-hot representation\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 モデル定義\n",
    "\n",
    "kerasでは2通りのモデルの記述方法があります．\n",
    "\n",
    "- `Sequential`: 単純に入力層から出力層に向かって順番に層を追加してモデルを記述\n",
    "- `FunctionalAPI`: `Sequential`では記述できないような複雑なモデルを記述\n",
    "\n",
    "今回やるようなMNISTであれば前者で十分です．\n",
    "\n",
    "この例であれば，\n",
    "- 入力層784次元\n",
    "- 中間層128次元\n",
    "- 中間層の活性化関数が`reLU`関数\n",
    "- 2割のユニットをDropout\n",
    "- 出力層10次元\n",
    "- 出力層の活性化関数が`softmax`関数\n",
    "\n",
    "を記述します．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Sequentialの書き方\n",
    "\n",
    "- `Sequential`を用意\n",
    "- `add`で中間層や活性化関数を追加\n",
    "- `Dropout`も同じように`add`で追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=input_unit_size, init='glorot_uniform'))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(p=0.2))\n",
    "model.add(Dense(nb_classes, init='glorot_uniform'))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 FunctinalAPIの書き方\n",
    "\n",
    "- 入力データを受け取る`inputs`と，層の関数を繋げて目的値を得る`outputs`までを定義\n",
    "- `Model`に`inputs`と`outputs`を渡す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(input_unit_size,))\n",
    "x = Dense(128, activation='relu')(inputs)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(nb_classes, activation=\"softmax\")(x)\n",
    "model = Model(input=inputs, output=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 モデル可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 最適化\n",
    "\n",
    "`SGD`のほかにも`AdaDelta`などがあります．\n",
    "\n",
    "デフォルトのパラメータだけ指定する場合は，省略可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adadelta()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 学習\n",
    "\n",
    "モデルの記述と最適化の定義が終わったら，モデルのコンパイルを行います．\n",
    "\n",
    "`compile`関数でロス関数，最適化関数，評価尺度などを指定します．\n",
    "\n",
    "学習はバッチ処理など，複数の学習方法があります．  \n",
    "もっと単純な`fit`では`scikit-learn`風に学習データを渡すだけで学習が進みます．\n",
    "\n",
    "便利機能として学習経過を表示してくれます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "nb_epoch = 5\n",
    "\n",
    "# 学習\n",
    "result = model.fit(X_train, Y_train, nb_epoch=nb_epoch, batch_size=256, verbose=2,  validation_split=0.2)\n",
    "\n",
    "# テストデータでの予測\n",
    "score = model.evaluate(X_test, Y_test, batch_size=256)\n",
    "print(\"test loss:\", score[0])\n",
    "print(\"test accu:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 epochごとのlossとaccuracy可視化\n",
    "\n",
    "学習が終わると`History`オブジェクトを返します．  \n",
    "モデルのパラメータのほかにepochごとのlossやaccuracyが記録されています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(nb_epoch)\n",
    "plt.plot(x, result.history['acc'], label=\"train acc\")\n",
    "plt.plot(x, result.history['val_acc'], label=\"val acc\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, result.history['loss'], label=\"train loss\")\n",
    "plt.plot(x, result.history['val_loss'], label=\"val loss\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 モデルの保存と読み込み\n",
    "kerasのモデルファイルは`json`か`yaml`形式，\n",
    "学習したモデルのパラメータは`HDF5`形式で保存・読み込み可能です．\n",
    "\n",
    "モデルの読み込みは，以下の手順で行います．\n",
    "\n",
    "1. モデルファイルを読み込む\n",
    "2. パラメータファイルを読み込み，モデルに代入\n",
    "3. `compile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model without weights\n",
    "with open('mnist_model.json', 'w') as f:\n",
    "    json.dump(model.to_json(), f)\n",
    "    \n",
    "model.save_weights('mnist_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "mnist_model = model_from_json(json.load(open(\"mnist_model.json\")))\n",
    "\n",
    "# load wights\n",
    "mnist_model.load_weights(\"./mnist_weights.h5\")\n",
    "mnist_model.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 まとめ\n",
    "\n",
    "kerasを使ったNNの学習の流れは以下のようになります．\n",
    "\n",
    "1. データを用意\n",
    "2. model記述\n",
    "3. modelコンパイル\n",
    "4. training！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. CNN with MNIST\n",
    "\n",
    "画像処理で使われるCNNについてMNISTを例にして紹介します．  \n",
    "MLPとほぼ同じようにモデルの記述ができます．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 データ\n",
    "\n",
    "CNNの入力画像は，$(channels, rows, columns)$というテンソルを使います．  \n",
    "MNISTはグレースケールなので$(1, 28, 28)$となります．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols = 28, 28\n",
    "nb_classes = 10\n",
    "nb_filters = 10 # the number of filters\n",
    "nb_pool = 2 # window size of pooling\n",
    "nb_conv = 3 # window size of filter\n",
    "\n",
    "# data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols) \n",
    "X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 モデル定義\n",
    "MLPと同じく畳み込み層や`pooling`層を`add`で積んでいきます．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(nb_filters, nb_conv, nb_conv, input_shape=(1, img_rows, img_cols)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(nb_filters, nb_conv, nb_conv))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=\"adadelta\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 5\n",
    "cnn_result = model.fit(X_train, Y_train, nb_epoch=nb_epoch, batch_size=256, verbose=2,  validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 学習曲線の可視化\n",
    "MLPの結果と比較してみましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(nb_epoch)\n",
    "plt.plot(x, result.history['acc'], label=\"train acc\")\n",
    "plt.plot(x, result.history['val_acc'], label=\"val acc\")\n",
    "plt.plot(x, cnn_result.history['acc'], label=\"cnn train acc\")\n",
    "plt.plot(x, cnn_result.history['val_acc'], label=\"cnn val acc\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, result.history['loss'], label=\"train loss\")\n",
    "plt.plot(x, result.history['val_loss'], label=\"val loss\")\n",
    "plt.plot(x, cnn_result.history['loss'], label=\"cnn train loss\")\n",
    "plt.plot(x, cnn_result.history['val_loss'], label=\"cnn val loss\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 中間層の可視化\n",
    "kerasでは，定義したモデルの各層の入出力を関数として扱えます．  \n",
    "それを用いて，MNISTの入力画像に対して中間層がどのような画像に変換されているかを可視化します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1元画像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_size = 10\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "for i in range(show_size):\n",
    "    draw_digit(X_train[i].reshape(28,28), 1, show_size, i+1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 中間層\n",
    "最初の畳み込み層に`relu`を通した画像の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_first_layer_output = K.function([model.layers[0].input],\n",
    "                                    [model.layers[1].output])\n",
    "first_layer = get_first_layer_output([X_train[0:show_size]])[0]\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "for img_index, filters in enumerate(first_layer, start=1):\n",
    "    for filter_index, mat in enumerate(filters):\n",
    "        pos = (filter_index)*10+img_index\n",
    "        draw_digit(mat, nb_filters, show_size, pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.3 中間層\n",
    "2つ目の畳み込み層に`relu`を通した画像の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_second_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[3].output])\n",
    "second_output = get_second_layer_output([X_train[0:show_size]])[0]\n",
    "\n",
    "show_size = 10\n",
    "plt.figure(figsize=(20,20))\n",
    "print(second_output.shape)\n",
    "\n",
    "for img_index, filters in enumerate(second_output, start=1):\n",
    "    for filter_index, mat in enumerate(filters):\n",
    "        pos = (filter_index)*10+img_index\n",
    "        draw_digit(mat, nb_filters, show_size, pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.4 pooling層\n",
    "`maxpooling`を通した画像の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_3rd_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[4].output])\n",
    "layers = get_3rd_layer_output([X_train[0:show_size]])[0]\n",
    "\n",
    "show_size = 10\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "for img_index, filters in enumerate(layers, start=1):\n",
    "    for filter_index, mat in enumerate(filters):\n",
    "        pos = (filter_index)*10+img_index\n",
    "        draw_digit(mat, nb_filters, show_size, pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. CNN with CIFAR10\n",
    "今度はカラー画像を使ったCNNを扱います．  \n",
    "$(channels=3, rows=32, columns=32)$というテンソルで1つの画像を表現します．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 データ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "nb_classes = 10\n",
    "nb_epoch = 5\n",
    "nb_filter = 10\n",
    "\n",
    "img_rows, img_cols = 32, 32\n",
    "img_channels = 3\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 モデル定義と学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(nb_filter, 3, 3, input_shape=(img_channels, img_rows, img_cols)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(nb_filter, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "def draw_color(data, row, col, n):\n",
    "    plt.subplot(row, col, n)\n",
    "    plt.imshow(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 元画像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_size = 10\n",
    "plt.figure(figsize=(20,20))\n",
    "for i in range(show_size):\n",
    "    draw_color(X_train[i].reshape(3, 32, 32).transpose(1, 2, 0), 1, show_size, i+1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 畳み込み層+relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "layer_output = K.function([model.layers[0].input],\n",
    "                          [model.layers[1].output])\n",
    "layers = layer_output([X_train[0:show_size]])[0]\n",
    "for img_index, filters in enumerate(layers, start=1):\n",
    "    for filter_index, mat in enumerate(filters):\n",
    "        pos = (filter_index)*show_size+img_index\n",
    "        draw_color(mat, nb_filter, show_size, pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 畳み込み層+relu+畳み込み層+relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "layer_output = K.function([model.layers[0].input],\n",
    "                          [model.layers[3].output])\n",
    "\n",
    "layers = layer_output([X_train[0:show_size]])[0]\n",
    "\n",
    "for img_index, filters in enumerate(layers, start=1):\n",
    "    for filter_index, mat in enumerate(filters):\n",
    "        pos = (filter_index)*show_size+img_index\n",
    "        draw_color(mat, nb_filter, show_size, pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 畳み込み層+relu+畳み込み層+relu+maxPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "layer_output = K.function([model.layers[0].input],\n",
    "                          [model.layers[4].output])\n",
    "layers = layer_output([X_train[0:show_size]])[0]\n",
    "\n",
    "\n",
    "for img_index, filters in enumerate(layers, start=1):\n",
    "    for filter_index, mat in enumerate(filters):\n",
    "        pos = (filter_index)*show_size+img_index\n",
    "        mat_size = mat.shape[1]\n",
    "        draw_color(mat, nb_filter, show_size, pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. RNN with binary classification\n",
    "\n",
    "言語処理や音声など時系列データに使われるNNです．  \n",
    "`Embeddings`と`RNN`について触れます．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 データ\n",
    "\n",
    "映画のレビューデータを扱います．  \n",
    "入力が単語系列，出力が0か1の2値です．  \n",
    "特徴量として頻出語20,000単語だけを使い，1レビューは最長140単語とします．  \n",
    "このような前処理もkerasの関数に用意されています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n",
    "maxlen=140\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "nb_epoch = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 モデル定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_features, output_dim=100, init='glorot_uniform', input_length=maxlen))\n",
    "model.add(GRU(20, return_sequences=False))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Embedding layer\n",
    "\n",
    "1つの整数（例えば単語を数値に変化したもの）を1つのベクトルに対応づける層です．  \n",
    "上で定義したものは最大で140単語が含まれる文を入力し，それぞれの単語を100次元の単語ベクトルに変換します．  \n",
    "1単語が100次元でそれが最長140系列，それがデータ数だけあるという状態に対応します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_train[0:2]\n",
    "print(\"Inputs shape\", x.shape)\n",
    "\n",
    "embedding_layer = K.function([model.layers[0].input],[model.layers[0].output])\n",
    "print(\"Outputs shape\", embedding_layer([x])[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 RNN  layer\n",
    "\n",
    "\n",
    "kerasの`RNN`層は3種類用意されています．\n",
    "\n",
    "- `SimpleRNN`\n",
    "- `GRU`\n",
    "- `LSTM`\n",
    "\n",
    "テンソル（さっきの`Embedding`層を通した結果のデータと同じ次元を持つデータ）を入力とし，\n",
    "行列かテンソルを出力します．\n",
    "\n",
    "- 次の層もRNNであれば各時刻ごとに処理する必要があるので，テンソル\n",
    "- 次の層が出力層や固定長ベクトルを構築する場合，最後の時刻のRNNの出力さえあればいいので，行列\n",
    "\n",
    "という使い分けができるかと思います．  \n",
    "コードレベルでいうと  \n",
    "`RNN(return_sequences=True)`が前者，\n",
    "`RNN(return_sequences=False)`が後者に対応します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_gru_1 = model.fit(X_train, y_train, batch_size=256, nb_epoch=nb_epoch, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 more RNN\n",
    "`GRU`と`LSTM`を比較します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_features, output_dim=100, init='glorot_uniform', input_length=maxlen))\n",
    "model.add(GRU(20, return_sequences=True))\n",
    "model.add(GRU(20, return_sequences=False))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "res_gru_2 = model.fit(X_train, y_train, batch_size=256, nb_epoch=nb_epoch, validation_data=(X_test, y_test))\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_features, output_dim=100, init='glorot_uniform', input_length=maxlen))\n",
    "model.add(LSTM(20, return_sequences=False))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "res_lstm_1 = model.fit(X_train, y_train, batch_size=256, nb_epoch=nb_epoch,\n",
    "                    validation_data=(X_test, y_test))\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(nb_epoch)\n",
    "plt.plot(x, res_gru_1.history['acc'], label=\"GRU 1 train\")\n",
    "plt.plot(x, res_gru_1.history['val_acc'], label=\"GRU 1 val\")\n",
    "plt.plot(x, res_gru_2.history['acc'], label=\"GRU 2 train\")\n",
    "plt.plot(x, res_gru_2.history['val_acc'], label=\"GRU 2 val\")\n",
    "plt.plot(x, res_lstm_1.history['acc'], label=\"LSTM train\")\n",
    "plt.plot(x, res_lstm_1.history['val_acc'], label=\"LSTM val\")\n",
    "plt.title(\"binary accuracy\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, res_gru_1.history['loss'], label=\"GRU 1 train\")\n",
    "plt.plot(x, res_gru_1.history['val_loss'], label=\"GRU 1 val\")\n",
    "plt.plot(x, res_gru_2.history['loss'], label=\"GRU 2 train\")\n",
    "plt.plot(x, res_gru_2.history['val_loss'], label=\"GRU 2 val\")\n",
    "plt.plot(x, res_lstm_1.history['loss'], label=\"LSTM train\")\n",
    "plt.plot(x, res_lstm_1.history['val_loss'], label=\"LSTM val\")\n",
    "plt.title(\"binary loss\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Neural Network Language Models\n",
    "\n",
    "Markov modelsでやるような，前の単語系列から次の単語系列を予測を行います．\n",
    "\n",
    "## 5.1 データ\n",
    "_Alice's Adventures in Wonderland_ の一部を使います．  \n",
    "`get_file` では指定したURLのファイルを `~/.keras/datasets/`以下に保存できます．\n",
    "\n",
    "`Tokenizer`はテキストデータの前処理を行う便利クラスで，単語系列を数値の系列に変換します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = get_file('alice.txt', origin=\"http://www.gutenberg.org/cache/epub/11/pg11.txt\")\n",
    "doc = open(path).readlines()[0:50]\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(doc)\n",
    "doc = tokenizer.texts_to_sequences(doc)\n",
    "doc = [l for l in doc if len(l) > 1]\n",
    "words_size = sum([len(words) - 1 for words in doc])\n",
    "\n",
    "maxlen = max([len(x)-1 for x in doc])\n",
    "vocab_size = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 学習方法の工夫\n",
    "\n",
    "前の系列から次の1単語を予測するように学習\n",
    "\n",
    "1. _alice's_ -> _Adventures_\n",
    "2. _alice's Adventures_ -> _in_\n",
    "3.  _alice's Adventures in_ -> _..._\n",
    "\n",
    "真面目にこのデータを作るとメモリ不足になるのでpythonの `generator` を使います．  \n",
    "kerasでは`generator`を引数にとって学習することも可能であるが，nzwの環境ではうまくいかなかったので，\n",
    "ここでは，文単位で学習データをつくり，`train_on_batch`関数で学習を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(X, maxlen, V):\n",
    "    for sentence in X: \n",
    "        inputs = []\n",
    "        targets = []\n",
    "        for i in range(1, len(sentence)):\n",
    "            inputs.append(sentence[0:i])\n",
    "            targets.append(sentence[i])\n",
    "            y = np_utils.to_categorical(targets, V)\n",
    "        inputs_sequence = sequence.pad_sequences(inputs, maxlen=maxlen)\n",
    "        yield (inputs_sequence, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 モデル定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_units = 64\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, nb_units, input_length=maxlen))\n",
    "model.add(LSTM(nb_units, return_sequences=False))\n",
    "model.add(Dense(vocab_size))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 学習と文生成\n",
    "\n",
    "### 5.4.1 サンプリング方法\n",
    "\n",
    "1epochごとに _alice's_ を入力とし，\n",
    "次の単語を予測を行い，予測した単語を_alice's_に加えて新しい文とします．  \n",
    "これを最大系列長になるまで予測を繰り返し行います．\n",
    "\n",
    "次の単語は，単語の予測分布（多項分布）に従いサンプルします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(p):\n",
    "    p /= sum(p)\n",
    "    return np.where(np.random.multinomial(1,p,1)==1)[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(19):\n",
    "    for x, y in generate_data(doc, maxlen, vocab_size):\n",
    "        model.train_on_batch(x, y)\n",
    "\n",
    "    in_words = \"alice's\"\n",
    "    for _ in range(maxlen):\n",
    "        in_sequence = sequence.pad_sequences(tokenizer.texts_to_sequences([in_words]), maxlen=maxlen)\n",
    "        wordid = sample(model.predict(in_sequence)[0])\n",
    "        for k, v in tokenizer.word_index.items():\n",
    "            if v == wordid:\n",
    "                in_words += \" \" + k\n",
    "                break\n",
    "\n",
    "    print(i, in_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 最尤推定\n",
    "\n",
    "確率値最大となる単語を連結し，文生成を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_words = \"alice's\"\n",
    "for _ in range(maxlen):\n",
    "    in_sequence = sequence.pad_sequences(tokenizer.texts_to_sequences([in_words]), maxlen=maxlen)\n",
    "    wordid = model.predict_classes(in_sequence, verbose=0)[0]\n",
    "    for k, v in tokenizer.word_index.items():\n",
    "        if v == wordid:\n",
    "            in_words += \" \" + k\n",
    "            break\n",
    "\n",
    "print(in_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. encoder-decoder\n",
    "\n",
    "先ほどと類似したものとして，今度は入力系列を日本語，出力系列を英語とした翻訳を行うモデルです．  \n",
    "（kerasのサンプルでは，画像を入力としその説明文を出力するようなNNがあります）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 データ\n",
    "対訳コーパスや発言に対する返答など行対応のとれているファイルが必要になります．  \n",
    "ここでは _The Yellow Face_ の日英の文書を，[ここ](http://www2.nict.go.jp/univ-com/multi_trans/member/mutiyama/align/index.html) から持ってきてその一部を使います．  \n",
    "日本語については形態素解析済みとします．  \n",
    "翻訳先の文に対して，文の始まりを表す記号`GOS`，文末を表す記号`EOS`を追加して学習データを作ります．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = base_filter() + \"「」・。、（）？！　'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ja data\n",
    "ja_docs = open(\"data/yellow.ja.txt\").readlines()[0:50]\n",
    "ja_tokenizer = Tokenizer(filters=filters)\n",
    "ja_tokenizer.fit_on_texts(ja_docs)\n",
    "\n",
    "ja_docs = ja_tokenizer.texts_to_sequences(ja_docs)\n",
    "\n",
    "# en data\n",
    "en_docs = open(\"data/yellow.en.txt\").readlines()\n",
    "en_docs = list(map(lambda x: \"GOS \" + x + \" EOS\" , en_docs))[0:50]\n",
    "en_tokenizer = Tokenizer(filters=filters)\n",
    "en_tokenizer.fit_on_texts(en_docs)\n",
    "en_tokenizer.fit_on_texts(en_docs)\n",
    "\n",
    "en_docs = en_tokenizer.texts_to_sequences(en_docs)\n",
    "\n",
    "encoder_maxlen = max([len(x) for x in ja_docs])\n",
    "decoder_maxlen = max([len(x)-1 for x in en_docs])\n",
    "encoder_vocab_size = len(ja_tokenizer.word_index)+1\n",
    "decoder_vocab_size = len(en_tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 バッチ処理を行うための関数定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_training_data(X, Y, encoder_maxlen, decoder_maxlen, V, samples_size):\n",
    "    encoder_inputs = []\n",
    "    decoder_inputs = []\n",
    "    next_words = []\n",
    "    for i in range(len(X)):\n",
    "        x_doc, y_doc = X[i], Y[i]\n",
    "        for j in range(1, len(y_doc)):\n",
    "            decoder_inputs.append(y_doc[0:j])\n",
    "            next_words.append(y_doc[j])\n",
    "            encoder_inputs.append(x_doc)\n",
    "            if len(next_words) == samples_size:                \n",
    "                labels = np_utils.to_categorical(next_words, V)\n",
    "                encoder_inputs = sequence.pad_sequences(encoder_inputs, maxlen=encoder_maxlen)\n",
    "                decoder_inputs = sequence.pad_sequences(decoder_inputs, maxlen=decoder_maxlen)\n",
    "                yield ([encoder_inputs, decoder_inputs], labels)\n",
    "                encoder_inputs = []\n",
    "                decoder_inputs = []\n",
    "                next_words = []\n",
    "    labels = np_utils.to_categorical(next_words, V)\n",
    "    encoder_inputs = sequence.pad_sequences(encoder_inputs, maxlen=encoder_maxlen)\n",
    "    decoder_inputs = sequence.pad_sequences(decoder_inputs, maxlen=decoder_maxlen)\n",
    "\n",
    "    yield ([encoder_inputs, decoder_inputs], labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 モデル定義\n",
    "独立した`Sequential`を結合して新しい`Sequential`を構築できます．  \n",
    "定義するモデルは，基本的にNNLMと同じで1つ先の単語列を予測を行うようなNNです．\n",
    "\n",
    "- 翻訳元の言語の文から1つの中間ベクトルを出力する`encoder`\n",
    "- 翻訳先の予測する1単語目から`N-1`単語目までの単語を入力として受け取る`decoder_input`\n",
    "- `encoder`と`decoder_input`を結合して，1つの単語を予測を行う`model`\n",
    "\n",
    "の3つから構成される．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder one doc to one repeated vector\n",
    "encoder = Sequential()\n",
    "encoder.add(Embedding(encoder_vocab_size, 128, input_length=encoder_maxlen))\n",
    "encoder.add(GRU(128, return_sequences=False))\n",
    "encoder.add(RepeatVector(decoder_maxlen))\n",
    "\n",
    "# decoder inputs\n",
    "decoder_input = Sequential()\n",
    "decoder_input.add(Embedding(decoder_vocab_size, 128, input_length=decoder_maxlen))\n",
    "decoder_input.add(GRU(output_dim=128, return_sequences=True))\n",
    "decoder_input.add(TimeDistributed(Dense(128)))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([encoder, decoder_input], mode='concat', concat_axis=-1))\n",
    "model.add(GRU(128, return_sequences=False))\n",
    "model.add(Dense(decoder_vocab_size))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "\n",
    "with open('my_model.json', 'w') as f:\n",
    "    json.dump(model.to_json(), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"encoder\")\n",
    "display(SVG(model_to_dot(encoder, show_shapes=True).create(prog='dot', format='svg')))\n",
    "print(\"decoder input\")\n",
    "display(SVG(model_to_dot(decoder_input, show_shapes=True).create(prog='dot', format='svg')))\n",
    "print(\"merge and decoder output\")\n",
    "display(SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(21):\n",
    "        loss = 0.\n",
    "        for x, y in gen_training_data(ja_docs, en_docs, encoder_maxlen, decoder_maxlen, decoder_vocab_size, 256):\n",
    "            loss += model.train_on_batch(x, y)\n",
    "        print(i, \"loss \", loss)\n",
    "        if not i % 10:\n",
    "            model.save_weights('my_model_weights_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 翻訳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = model_from_json(json.load(open(\"my_model.json\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_words = \"ホームズ は 答え た\"\n",
    "encoder_in = sequence.pad_sequences(ja_tokenizer.texts_to_sequences([encoder_words]), maxlen=encoder_maxlen)\n",
    "\n",
    "for i in range(0, 21, 10):\n",
    "    model.load_weights(\"./my_model_weights_\" + str(i) + \".h5\")\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "\n",
    "    decoder_words = \"gos\"\n",
    "    for _ in range(encoder_maxlen):\n",
    "        decoder_in = sequence.pad_sequences(en_tokenizer.texts_to_sequences([decoder_words]),  maxlen=decoder_maxlen)\n",
    "        wordid = sample(model.predict([encoder_in, decoder_in])[0])\n",
    "        \n",
    "        for k, v in en_tokenizer.word_index.items():\n",
    "            if v == wordid:\n",
    "                decoder_words += \" \" + k\n",
    "                break\n",
    "        if k == \"eos\":\n",
    "            break\n",
    "\n",
    "    print(i, decoder_words.replace(\"gos\", \"\").replace(\"eos\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. autoencoder with MNIST\n",
    "\n",
    "入力画像を一旦中間的な表現に変換してからそれを入力画像に復元するように学習を行うモデルです．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 データ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_unit_size = 28*28\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], input_unit_size)[0:7000]\n",
    "X_train = X_train.astype('float32')\n",
    "X_train /= 255\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 モデル定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(input_unit_size,))\n",
    "x = Dense(144, activation='relu')(inputs)\n",
    "outputs = Dense(input_unit_size)(x)\n",
    "model = Model(input=inputs, output=outputs)\n",
    "model.compile(loss='mse', optimizer='adadelta')\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, X_train, nb_epoch=19, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 可視化\n",
    "### 7.4.1 入力画像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_size = 10\n",
    "total = 0\n",
    "plt.figure(figsize=(20,20))\n",
    "for i in range(show_size):\n",
    "    for j in range(show_size):    \n",
    "        draw_digit(X_train[total].reshape(28,28), show_size, show_size, total+1)\n",
    "        total+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.2 中間層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_layer_output = K.function([model.layers[0].input],\n",
    "                              [model.layers[1].output])\n",
    "\n",
    "hidden_outputs = get_layer_output([X_train[0:show_size**2]])[0]\n",
    "\n",
    "total = 0\n",
    "plt.figure(figsize=(20,20))\n",
    "for i in range(show_size):\n",
    "    for j in range(show_size):    \n",
    "        draw_digit(hidden_outputs[total].reshape(12, 12), show_size, show_size, total+1)\n",
    "        total+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4.3 復元画像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_layer_output = K.function([model.layers[0].input],\n",
    "                              [model.layers[2].output])\n",
    "\n",
    "last_outputs = get_layer_output([X_train[0:show_size**2]])[0]\n",
    "\n",
    "total = 0\n",
    "plt.figure(figsize=(20,20))\n",
    "for i in range(show_size):\n",
    "    for j in range(show_size):    \n",
    "        draw_digit(last_outputs[total].reshape(28, 28), show_size, show_size, total+1)\n",
    "        total+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Continuous Bag of Words\n",
    "word2vecで有名になったアルゴリズムの片方\n",
    "\n",
    "ある単語の周囲の単語ベクトルを入力とし，注目している単語を予測するように学習を行います．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 データ\n",
    "\n",
    "_Alice's Adventures in Wonderland_ を使います．  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_filter()\n",
    "path = get_file('alice.txt', origin=\"http://www.gutenberg.org/cache/epub/11/pg11.txt\")\n",
    "corpus = open(path).readlines()[0:500]\n",
    "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
    "\n",
    "tokenizer = Tokenizer(filters=base_filter()+\"'\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "corpus = tokenizer.texts_to_sequences(corpus)\n",
    "V = len(tokenizer.word_index) + 1 # 語彙数\n",
    "dim = 100 # ベクトルの次元\n",
    "window_size = 2 # 文脈窓幅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(corpus, window_size, V):\n",
    "    maxlen = window_size*2\n",
    "    for words in corpus:\n",
    "        contexts = []\n",
    "        labels   = []\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            s = index-window_size\n",
    "            e = index+window_size+1\n",
    "            \n",
    "            contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
    "            labels.append(word)\n",
    "\n",
    "            y = np_utils.to_categorical(labels, V)\n",
    "            x = sequence.pad_sequences(contexts, maxlen=maxlen)\n",
    "            yield (x, y)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 モデル定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=V, output_dim=dim, init='glorot_uniform',input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\n",
    "cbow.add(Dense(V, init='glorot_uniform', activation='softmax'))\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer=\"adadelta\")\n",
    "SVG(model_to_dot(cbow, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ite in range(10):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data(corpus, window_size, V):\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "    print(ite, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 単語ベクトルの保存\n",
    "\n",
    "単語ベクトルを`gensim`で扱うものとして，オリジナルの`word2vec`の単語ベクトルと同じ形式で保存します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('vectors.txt' ,'w')\n",
    "f.write(\" \".join([str(V-1), str(dim)]))\n",
    "f.write(\"\\n\")\n",
    "\n",
    "vectors = cbow.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 word similarity\n",
    "得られた単語ベクトルを元に単語間の類似度計算をしてみましょう．  \n",
    "例として `alice`と近い単語集合を示します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec.load_word2vec_format('./vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.most_similar(positive=['alice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Skip Gram\n",
    "word2vecで有名になったアルゴリズムのもう片方\n",
    "\n",
    "1単語を入力とし，その周囲で共起する単語を予測するように学習を行います．  \n",
    "negative-sampingや階層ソフトマックスは行いません．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 データ\n",
    "\n",
    "CBoWと同じく _Alice's Adventures in Wonderland_ を使います．  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_filter()\n",
    "path = get_file('alice.txt', origin=\"http://www.gutenberg.org/cache/epub/11/pg11.txt\")\n",
    "corpus = open(path).readlines()[0:500]\n",
    "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
    "\n",
    "tokenizer = Tokenizer(filters=base_filter()+\"'\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "corpus = tokenizer.texts_to_sequences(corpus)\n",
    "V = len(tokenizer.word_index) + 1 # 語彙数\n",
    "dim = 100 # ベクトルの次元\n",
    "window_size = 2 # 文脈窓幅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(corpus, window_size, V):\n",
    "    maxlen = window_size*2\n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            s = index-window_size\n",
    "            e = index+window_size+1\n",
    "                    \n",
    "            in_words = []\n",
    "            labels = []\n",
    "            for i in range(s, e):\n",
    "                if i != index and 0 <= i < L:\n",
    "                    in_words.append([word] )\n",
    "                    labels.append(words[i])\n",
    "\n",
    "            x = np.array(in_words,dtype=np.int32)\n",
    "            y = np_utils.to_categorical(labels, V)\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 モデル定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram = Sequential()\n",
    "skipgram.add(Embedding(input_dim=V, output_dim=dim, init='glorot_uniform', input_length=1))\n",
    "skipgram.add(Reshape((dim, )))\n",
    "skipgram.add(Dense(input_dim=dim, output_dim=V, activation='softmax'))\n",
    "skipgram.compile(loss='categorical_crossentropy', optimizer=\"adadelta\")\n",
    "SVG(model_to_dot(skipgram, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ite in range(10):\n",
    "    print(ite)\n",
    "    for x, y in generate_data(corpus, window_size, V):\n",
    "        skipgram.train_on_batch(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 単語ベクトルの保存とword similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('vectors.txt' ,'w')\n",
    "f.write(\" \".join([str(V-1),str(dim)]))\n",
    "f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = skipgram.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec.load_word2vec_format('./vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.most_similar(positive=['alice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 10. リソース\n",
    "\n",
    "知ってる範囲でkeras関連の情報を列挙します．\n",
    "\n",
    "- http://keras.io/\n",
    "- [日本語docs(作業が始まったばかり)](https://github.com/fchollet/keras-docs-ja)\n",
    "- [blog](http://blog.keras.io/)\n",
    "- slack\n",
    "- [google group](https://groups.google.com/forum/#!forum/keras-users)\n",
    "- [github](https://github.com/fchollet/keras)\n",
    "- [examples](https://github.com/fchollet/keras/tree/master/examples)\n",
    "- [youtube](https://www.youtube.com/results?search_query=keras+deep+learning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
